{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48459ac",
   "metadata": {},
   "source": [
    "## Lab 6, Group 2\n",
    "### Names: Hailey DeMark, Deborah Park, Karis Park\n",
    "### Student IDs: 48869449, 48878679, 48563429\n",
    "\n",
    "Dataset: (Link to dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a87e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c484088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4514 images.\n",
      "X shape: (4514, 64, 64, 1)\n",
      "Encoded labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# define dataset directory\n",
    "dataset_path = 'Brain Tumor Data Set' \n",
    "\n",
    "# collect all image paths and labels\n",
    "image_paths = []\n",
    "image_labels = []\n",
    "\n",
    "for root, _, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_paths.append(os.path.join(root, file))\n",
    "            image_labels.append(os.path.basename(root)) \n",
    "\n",
    "print(f\"Found {len(image_paths)} images.\")\n",
    "\n",
    "# load and preprocess images in grayscale (size 64x64)\n",
    "image_data = []\n",
    "for file in image_paths:\n",
    "    try:\n",
    "        image = Image.open(file).convert('L')         # convert to grayscale\n",
    "        image = image.resize((64, 64))                # resize to fixed size\n",
    "        image_array = np.array(image)                 # convert to NumPy array\n",
    "        image_data.append(image_array)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "# convert list to NumPy array and normalize pixel values\n",
    "X = np.array(image_data, dtype='float32') / 255.0\n",
    "X = X.reshape(-1, 64, 64, 1)  # Add grayscale channel dimension\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "\n",
    "# encode text labels to binary integers (e.g., yes = 1, no = 0)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(image_labels)\n",
    "print(\"Encoded labels:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba5b6e",
   "metadata": {},
   "source": [
    "### Preparation (3 points total)  \n",
    "* [1.5 points] Choose and explain what metric(s) you will use to evaluate your algorithm’s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "\n",
    "    * We will use recall as our evaluation metric. Recall is the best metric to evaluate our algorithm's performance since our dataset show MRI scans of potential brain tumors. In this case, a false negative (incorrectly identifying that a tumor is not present) can be extremely serious. Failing to detect a brain tumor can lead to missed or delayed treatment, which would lead to more severe conditions or death. While a false positive may cause unnecessary procedures, more testing, and emotional distress, it is significantly less harmful than overlooking a real tumor. Using recall ensures that the model is optimized to catch as many true tumor cases as possible, even at the expense of preidicting more false positives cases. In the context of medical diagnostics, especially for conditions as serious as brain cancer, it is better to err on the side of caution rather than risk missing anything.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f0ab3",
   "metadata": {},
   "source": [
    "* [1.5 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "\n",
    "    *  (I asked AI to verify if we could do a stratified 80/20 split for this lab like our last lab since I wasn't sure how big the dataset should be to be considered big enough to use this split...AI said a stratified 80/20 split is best with another stratified 80/20 split on the training data. I explain more in my explantation, but feel free to change!!)\n",
    "    \n",
    "\n",
    "    * We will use a stratified 80/20 split to separate the data into training and test sets, ensuring that the test set remains unseen throughout training and tuning. From the 80% training set, we will then apply another stratified 80/20 split to create a separate validation set so that 64% of the data is traiing data, 16% is validation data, and 20% is test data. This two-step stratified approach is ideal because it ensures that all subsets maintain a consistent proportion of tumor and non-tumor images and it ensures balanced performance metrics. Our dataset has about 55% cancerous images and 44% non-cancerous images, so by using a stratified split, this ensures that each split will reflect the original distribution of the two cases of images. The separate validation set will allow for hyperparameter tuning and modifying the model without touching the test set, and also helps to avoid overfitting to test metrics. In our case, K-fold cross-validation is computationally expensive due to training CNNs and our large dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482a6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First split: training vs test\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Second split: training vs validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a5cecb",
   "metadata": {},
   "source": [
    "### Modeling (6 points total)\n",
    "* [1.5 points]  Setup the training to use data expansion in Keras (also called data augmentation). Explain why the chosen data expansion techniques are appropriate for your dataset. You should make use of Keras augmentation layers, like in the class examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b2eec",
   "metadata": {},
   "source": [
    "    * (add in explaination about Data Expansion in Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67050926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, RandomContrast\n",
    "from tensorflow.keras.layers import RandomContrast, RandomTranslation, RandomZoom\n",
    "from tensorflow.keras.layers import RandomRotation, RandomFlip\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b40e1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - Precision: 0.5268 - Recall: 0.5209 - accuracy: 0.5457 - loss: 0.6858 - val_Precision: 0.7438 - val_Recall: 0.4521 - val_accuracy: 0.6750 - val_loss: 0.6086\n",
      "Epoch 2/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - Precision: 0.6912 - Recall: 0.5930 - accuracy: 0.6782 - loss: 0.6165 - val_Precision: 0.8571 - val_Recall: 0.5569 - val_accuracy: 0.7524 - val_loss: 0.5441\n",
      "Epoch 3/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - Precision: 0.7329 - Recall: 0.6517 - accuracy: 0.7194 - loss: 0.5530 - val_Precision: 0.7730 - val_Recall: 0.7036 - val_accuracy: 0.7676 - val_loss: 0.5097\n",
      "Epoch 4/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - Precision: 0.7366 - Recall: 0.6786 - accuracy: 0.7288 - loss: 0.5430 - val_Precision: 0.7604 - val_Recall: 0.8174 - val_accuracy: 0.7967 - val_loss: 0.4403\n",
      "Epoch 5/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - Precision: 0.8063 - Recall: 0.7572 - accuracy: 0.7972 - loss: 0.4546 - val_Precision: 0.7726 - val_Recall: 0.8443 - val_accuracy: 0.8133 - val_loss: 0.4051\n",
      "Epoch 6/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - Precision: 0.8165 - Recall: 0.7742 - accuracy: 0.8090 - loss: 0.4273 - val_Precision: 0.7857 - val_Recall: 0.8892 - val_accuracy: 0.8368 - val_loss: 0.3760\n",
      "Epoch 7/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - Precision: 0.8343 - Recall: 0.8076 - accuracy: 0.8316 - loss: 0.3825 - val_Precision: 0.7549 - val_Recall: 0.9311 - val_accuracy: 0.8285 - val_loss: 0.4041\n",
      "Epoch 8/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - Precision: 0.8262 - Recall: 0.7981 - accuracy: 0.8229 - loss: 0.3728 - val_Precision: 0.7530 - val_Recall: 0.9491 - val_accuracy: 0.8326 - val_loss: 0.3633\n",
      "Epoch 9/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - Precision: 0.8457 - Recall: 0.8372 - accuracy: 0.8494 - loss: 0.3284 - val_Precision: 0.7990 - val_Recall: 0.9281 - val_accuracy: 0.8589 - val_loss: 0.3144\n",
      "Epoch 10/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - Precision: 0.8620 - Recall: 0.8566 - accuracy: 0.8661 - loss: 0.3257 - val_Precision: 0.7809 - val_Recall: 0.9281 - val_accuracy: 0.8465 - val_loss: 0.3394\n",
      "Epoch 11/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - Precision: 0.8938 - Recall: 0.8677 - accuracy: 0.8877 - loss: 0.2875 - val_Precision: 0.7483 - val_Recall: 0.9701 - val_accuracy: 0.8354 - val_loss: 0.3733\n",
      "Epoch 12/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - Precision: 0.8653 - Recall: 0.8605 - accuracy: 0.8693 - loss: 0.2913 - val_Precision: 0.7704 - val_Recall: 0.9341 - val_accuracy: 0.8409 - val_loss: 0.3452\n",
      "Epoch 13/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - Precision: 0.8902 - Recall: 0.8906 - accuracy: 0.8955 - loss: 0.2475 - val_Precision: 0.7136 - val_Recall: 0.9551 - val_accuracy: 0.8022 - val_loss: 0.4042\n",
      "Epoch 14/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - Precision: 0.8875 - Recall: 0.8918 - accuracy: 0.8945 - loss: 0.2641 - val_Precision: 0.7887 - val_Recall: 0.9611 - val_accuracy: 0.8631 - val_loss: 0.3324\n",
      "Epoch 15/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - Precision: 0.8970 - Recall: 0.8870 - accuracy: 0.8976 - loss: 0.2542 - val_Precision: 0.8306 - val_Recall: 0.9251 - val_accuracy: 0.8783 - val_loss: 0.2608\n",
      "Epoch 16/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - Precision: 0.9064 - Recall: 0.8974 - accuracy: 0.9065 - loss: 0.2301 - val_Precision: 0.7199 - val_Recall: 0.9311 - val_accuracy: 0.8008 - val_loss: 0.4807\n",
      "Epoch 17/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - Precision: 0.9100 - Recall: 0.9183 - accuracy: 0.9177 - loss: 0.2092 - val_Precision: 0.8175 - val_Recall: 0.9251 - val_accuracy: 0.8700 - val_loss: 0.2820\n",
      "Epoch 18/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - Precision: 0.9145 - Recall: 0.9113 - accuracy: 0.9166 - loss: 0.1966 - val_Precision: 0.7672 - val_Recall: 0.9371 - val_accuracy: 0.8396 - val_loss: 0.3759\n",
      "Epoch 19/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - Precision: 0.9235 - Recall: 0.9309 - accuracy: 0.9302 - loss: 0.1761 - val_Precision: 0.7683 - val_Recall: 0.9731 - val_accuracy: 0.8520 - val_loss: 0.3887\n",
      "Epoch 20/20\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - Precision: 0.9335 - Recall: 0.9313 - accuracy: 0.9356 - loss: 0.1663 - val_Precision: 0.7292 - val_Recall: 0.9192 - val_accuracy: 0.8050 - val_loss: 0.4319\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation Layer\n",
    "data_augmentation = Sequential([\n",
    "    RandomFlip(\"horizontal\"),         # Reflect common real-world scan orientation variance\n",
    "    RandomRotation(0.05),             # Small tilts from patient positioning\n",
    "    RandomZoom(0.1),                  # Variance in zoom/focus during imaging\n",
    "    RandomTranslation(0.1, 0.1),      # Simulate slight patient movement\n",
    "    RandomContrast(0.1)               # Reflect scanner lighting/contrast differences\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "# CNN Model with augmentation included\n",
    "model = Sequential()\n",
    "model.add(data_augmentation)  # Augmentation applied during training only\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary output for tumor classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['Recall', 'Precision', 'accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056601f",
   "metadata": {},
   "source": [
    "* [2 points] Create a convolutional neural network to use on your data using Keras. Investigate at least two different convolutional network architectures and investigate changing one or more parameters of each architecture such as the number of filters. This means, at a  minimum, you will train a total of four models (2 different architectures, with 2 parameters changed in each architecture). Use the method of train/test splitting and evaluation metric that you argued for at the beginning of the lab. Visualize the performance of the training and validation sets per iteration (use the \"history\" parameter of Keras). Be sure that models converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa24a4",
   "metadata": {},
   "source": [
    "* [1.5 points] Visualize the final results of all the CNNs and interpret/compare the performances. Use proper statistics as appropriate, especially for comparing models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d562717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d15d6",
   "metadata": {},
   "source": [
    "* [1 points] Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve. Use proper statistical comparison techniques.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd1b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689caf7",
   "metadata": {},
   "source": [
    "### Exceptional Work (1 points total)\n",
    "You have free reign to provide additional analyses. \n",
    "One idea (required for 7000 level students): Use transfer learning with pre-trained weights for your initial layers of your CNN. Compare the performance when using transfer learning to your best model from above in terms of classification performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e8b3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
